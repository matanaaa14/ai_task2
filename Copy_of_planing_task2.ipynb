{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHQ1vGT/uYFDWJKWladjJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matanaaa14/ai_task2/blob/main/Copy_of_planing_task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0JQKG36CM9o",
        "outputId": "7244081f-436f-487a-cc49-b36d38188d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy 1 Score: -4.394\n",
            "Policy 2 Score: -4.913\n",
            "Policy 1 is better.\n",
            "\n",
            "Policy 1:\n",
            "→ ↓ ↓ ↓ ↓\n",
            "→ ↓ ↓ ← ↓\n",
            "→ → X ← ↑\n",
            "↑ → ↑ ← ↓\n",
            "↑ ← ↑ → G\n",
            "\n",
            "Policy 2:\n",
            "↓ ↓ ↓ ← ↓\n",
            "→ ↓ ↓ ← ↓\n",
            "→ → X ← ↓\n",
            "→ ↑ ↑ ↑ ↓\n",
            "↑ ← → → G\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameRL:\n",
        "    def __init__(self, rows, cols, rewards, move_probabilities, costs, goal, avoid, gamma=0.9, epsilon=0.1):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.probabilities = move_probabilities  # Dictionary of move: probability\n",
        "        self.costs = costs  # Cost matrix\n",
        "        self.goal = goal  # Goal location\n",
        "        self.avoid = avoid  # Avoid location\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_values = np.zeros((rows, cols, 4))  # Q-values for each state-action pair\n",
        "        self.returns = {}  # Dictionary to store returns for state-action pairs\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Possible moves: right, left, down, up\n",
        "\n",
        "        # Initialize returns dictionary\n",
        "        for row in range(rows):\n",
        "            for col in range(cols):\n",
        "                for action in range(4):\n",
        "                    self.returns[((row, col), action)] = []\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return 0 <= location[0] < self.rows and 0 <= location[1] < self.cols\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(range(4))  # Explore: choose a random action\n",
        "        else:\n",
        "            return np.argmax(self.q_values[state[0], state[1]])  # Exploit: choose the best action\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state  # Invalid move: stay in the same state\n",
        "        return next_state\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = [0, 0]  # Start at the initial position\n",
        "        while state != self.goal and state != self.avoid:\n",
        "            action = self.choose_action(state)\n",
        "            next_state = self.take_action(state, action)\n",
        "            reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "        return episode\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            if (tuple(state), action) not in visited:\n",
        "                self.returns[(tuple(state), action)].append(G)\n",
        "                self.q_values[state[0], state[1], action] = np.mean(self.returns[(tuple(state), action)])\n",
        "                visited.add((tuple(state), action))\n",
        "\n",
        "    def train(self, episodes=1000):\n",
        "        for episode_num in range(episodes):\n",
        "            episode = self.generate_episode()\n",
        "            self.update_q_values(episode)\n",
        "\n",
        "    def evaluate_policy(self, episodes=100):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while state != self.goal and state != self.avoid:\n",
        "                action = self.choose_action(state)\n",
        "                next_state = self.take_action(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] == self.goal:\n",
        "                    policy[row][col] = 'G'\n",
        "                elif [row, col] == self.avoid:\n",
        "                    policy[row][col] = 'X'\n",
        "                else:\n",
        "                    best_action = np.argmax(self.q_values[row, col])\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "# Example setup:\n",
        "rows = 5\n",
        "cols = 5\n",
        "rewards = [[0 for _ in range(cols)] for _ in range(rows)]  # Example reward matrix\n",
        "rewards[4][4] = 10  # Goal reward\n",
        "rewards[2][2] = -10  # Avoid penalty\n",
        "\n",
        "probability = {\n",
        "    (0, 1): 0.25,  # 25% chance to move right\n",
        "    (0, -1): 0.25, # 25% chance to move left\n",
        "    (1, 0): 0.25,  # 25% chance to move down\n",
        "    (-1, 0): 0.25  # 25% chance to move up\n",
        "}\n",
        "\n",
        "costs = [[-1 for _ in range(cols)] for _ in range(rows)]  # Example cost matrix\n",
        "\n",
        "goal = [4, 4]\n",
        "avoid = [2, 2]\n",
        "\n",
        "# Train and evaluate the first policy\n",
        "game_rl_1 = GridGameRL(rows, cols, rewards, probability, costs, goal, avoid)\n",
        "game_rl_1.train(episodes=10000)\n",
        "policy_1_score = game_rl_1.evaluate_policy(episodes=1000)\n",
        "print(\"Policy 1 Score:\", policy_1_score)\n",
        "\n",
        "# Train and evaluate the second policy\n",
        "game_rl_2 = GridGameRL(rows, cols, rewards, probability, costs, goal, avoid)\n",
        "game_rl_2.epsilon = 0.2  # Changing epsilon for the second policy as an example\n",
        "game_rl_2.train(episodes=10000)\n",
        "policy_2_score = game_rl_2.evaluate_policy(episodes=1000)\n",
        "print(\"Policy 2 Score:\", policy_2_score)\n",
        "\n",
        "# Compare policies\n",
        "if policy_1_score > policy_2_score:\n",
        "    print(\"Policy 1 is better.\")\n",
        "elif policy_1_score < policy_2_score:\n",
        "    print(\"Policy 2 is better.\")\n",
        "else:\n",
        "    print(\"Both policies are equally good.\")\n",
        "\n",
        "# Print policies for visual comparison\n",
        "print(\"\\nPolicy 1:\")\n",
        "game_rl_1.print_policy()\n",
        "print(\"\\nPolicy 2:\")\n",
        "game_rl_2.print_policy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probabilities, costs, goal, avoid, walls, gamma=0.9, epsilon=0.1):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probabilities = success_probabilities  # Dictionary of move: success probability\n",
        "        self.costs = costs  # Cost matrix\n",
        "        self.goal = goal  # Goal location\n",
        "        self.avoid = avoid  # Avoid location\n",
        "        self.walls = walls  # List of wall locations\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_values = np.zeros((rows, cols, 4))  # Q-values for each state-action pair\n",
        "        self.returns = {}  # Dictionary to store returns for state-action pairs\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Possible moves: right, left, down, up\n",
        "\n",
        "        # Initialize returns dictionary\n",
        "        for row in range(rows):\n",
        "            for col in range(cols):\n",
        "                for action in range(4):\n",
        "                    self.returns[((row, col), action)] = []\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(range(4))  # Explore: choose a random action\n",
        "        else:\n",
        "            return np.argmax(self.q_values[state[0], state[1]])  # Exploit: choose the best action\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        success_prob = self.success_probabilities[move]\n",
        "        if random.uniform(0, 1) < success_prob:\n",
        "            next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "            if not self.is_valid_location(next_state):\n",
        "                next_state = state  # Invalid move: stay in the same state\n",
        "        else:\n",
        "            next_state = state  # Move failed: stay in the same state\n",
        "        return next_state\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = [0, 0]  # Start at the initial position\n",
        "        while state != self.goal and state != self.avoid:\n",
        "            action = self.choose_action(state)\n",
        "            next_state = self.take_action(state, action)\n",
        "            reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "        return episode\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            if (tuple(state), action) not in visited:\n",
        "                self.returns[(tuple(state), action)].append(G)\n",
        "                self.q_values[state[0], state[1], action] = np.mean(self.returns[(tuple(state), action)])\n",
        "                visited.add((tuple(state), action))\n",
        "\n",
        "    def train(self, episodes=1000):\n",
        "        for episode_num in range(episodes):\n",
        "            episode = self.generate_episode()\n",
        "            self.update_q_values(episode)\n",
        "\n",
        "    def evaluate_policy(self, episodes=100):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while state != self.goal and state != self.avoid:\n",
        "                action = self.choose_action(state)\n",
        "                next_state = self.take_action(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] == self.goal:\n",
        "                    policy[row][col] = 'G'\n",
        "                elif [row, col] == self.avoid:\n",
        "                    policy[row][col] = 'X'\n",
        "                elif [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'\n",
        "                else:\n",
        "                    best_action = np.argmax(self.q_values[row, col])\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "# Example setup:\n",
        "rows = 5\n",
        "cols = 5\n",
        "rewards = [[0 for _ in range(cols)] for _ in range(rows)]  # Example reward matrix\n",
        "rewards[4][4] = 10  # Goal reward\n",
        "rewards[2][2] = -10  # Avoid penalty\n",
        "\n",
        "success_probabilities = {\n",
        "    (0, 1): 0.8,  # 80% chance to move right\n",
        "    (0, -1): 0.8, # 80% chance to move left\n",
        "    (1, 0): 0.8,  # 80% chance to move down\n",
        "    (-1, 0): 0.8  # 80% chance to move up\n",
        "}\n",
        "\n",
        "costs = [[-1 for _ in range(cols)] for _ in range(rows)]  # Example cost matrix\n",
        "\n",
        "goal = [4, 4]\n",
        "avoid = [2, 2]\n",
        "walls = [[1, 1], [1, 2], [2, 1]]  # Example walls\n",
        "\n",
        "# Train and evaluate the first policy\n",
        "game_rl_1 = GridGameRL(rows, cols, rewards, success_probabilities, costs, goal, avoid, walls)\n",
        "game_rl_1.train(episodes=10000)\n",
        "policy_1_score = game_rl_1.evaluate_policy(episodes=1000)\n",
        "print(\"Policy 1 Score:\", policy_1_score)\n",
        "\n",
        "# Train and evaluate the second policy\n",
        "game_rl_2 = GridGameRL(rows, cols, rewards, success_probabilities, costs, goal, avoid, walls)\n",
        "game_rl_2.epsilon = 0.2  # Changing epsilon for the second policy as an example\n",
        "game_rl_2.train(episodes=10000)\n",
        "policy_2_score = game_rl_2.evaluate_policy(episodes=1000)\n",
        "print(\"Policy 2 Score:\", policy_2_score)\n",
        "\n",
        "# Compare policies\n",
        "if policy_1_score > policy_2_score:\n",
        "    print(\"Policy 1 is better.\")\n",
        "elif policy_1_score < policy_2_score:\n",
        "    print(\"Policy 2 is better.\")\n",
        "else:\n",
        "    print(\"Both policies are equally good.\")\n",
        "\n",
        "# Print policies for visual comparison\n",
        "print(\"\\nPolicy 1:\")\n",
        "game_rl_1.print_policy()\n",
        "print(\"\\nPolicy 2:\")\n",
        "game_rl_2.print_policy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoVG9TD-CPeM",
        "outputId": "8a03cd55-1149-4bed-fb0a-79ccbbc33565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy 1 Score: -8.422\n",
            "Policy 2 Score: -9.555\n",
            "Policy 1 is better.\n",
            "\n",
            "Policy 1:\n",
            "↓ ← ← ↑ ↓\n",
            "↓ W W ↓ ↓\n",
            "↓ W X ← ←\n",
            "→ → ↑ ↑ ↓\n",
            "↑ ↑ → → G\n",
            "\n",
            "Policy 2:\n",
            "↓ ← ← ↓ ↓\n",
            "↓ W W ↑ ↓\n",
            "↓ W X ← ↓\n",
            "→ → ↑ ← ↓\n",
            "→ → ↑ → G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probabilities, costs, walls, gamma=0.9, epsilon=0.1):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probabilities = success_probabilities  # Dictionary of move: success probability\n",
        "        self.costs = costs  # Cost matrix\n",
        "        self.walls = walls  # List of wall locations\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_values = np.zeros((rows, cols, 4))  # Q-values for each state-action pair\n",
        "        self.returns = {}  # Dictionary to store returns for state-action pairs\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Possible moves: right, left, down, up\n",
        "\n",
        "        # Initialize returns dictionary\n",
        "        for row in range(rows):\n",
        "            for col in range(cols):\n",
        "                for action in range(4):\n",
        "                    self.returns[((row, col), action)] = []\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(range(4))  # Explore: choose a random action\n",
        "        else:\n",
        "            return np.argmax(self.q_values[state[0], state[1]])  # Exploit: choose the best action\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        success_prob = self.success_probabilities[move]\n",
        "        if random.uniform(0, 1) < success_prob:\n",
        "            next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "            if not self.is_valid_location(next_state):\n",
        "                next_state = state  # Invalid move: stay in the same state\n",
        "        else:\n",
        "            next_state = state  # Move failed: stay in the same state\n",
        "        return next_state\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = [0, 0]  # Start at the initial position\n",
        "        while True:\n",
        "            action = self.choose_action(state)\n",
        "            next_state = self.take_action(state, action)\n",
        "            reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "            episode.append((state, action, reward))\n",
        "            if self.rewards[state[0]][state[1]] != 0:\n",
        "                break\n",
        "            state = next_state\n",
        "        return episode\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            if (tuple(state), action) not in visited:\n",
        "                self.returns[(tuple(state), action)].append(G)\n",
        "                self.q_values[state[0], state[1], action] = np.mean(self.returns[(tuple(state), action)])\n",
        "                visited.add((tuple(state), action))\n",
        "\n",
        "    def train(self, episodes=1000):\n",
        "        for episode_num in range(episodes):\n",
        "            episode = self.generate_episode()\n",
        "            self.update_q_values(episode)\n",
        "\n",
        "    def evaluate_policy(self, episodes=100):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while True:\n",
        "                action = self.choose_action(state)\n",
        "                next_state = self.take_action(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                if self.rewards[state[0]][state[1]] != 0:\n",
        "                    break\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = '+{}'.format(int(self.rewards[row][col]))\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = '{}'.format(int(self.rewards[row][col]))\n",
        "                else:\n",
        "                    best_action = np.argmax(self.q_values[row, col])\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "# Example setup:\n",
        "rows = 4\n",
        "cols = 3\n",
        "rewards = [[0 for _ in range(cols)] for _ in range(rows)]  # Example reward matrix\n",
        "rewards[3][2] = 1  # Negative reward\n",
        "rewards[3][1] = -1  # Negative reward\n",
        "success_probabilities = {\n",
        "    (0, 1): 0.8,  # 80% chance to move right\n",
        "    (0, -1): 0.8, # 80% chance to move left\n",
        "    (1, 0): 0.8,  # 80% chance to move down\n",
        "    (-1, 0): 0.8  # 80% chance to move up\n",
        "}\n",
        "\n",
        "costs = [[-1 for _ in range(cols)] for _ in range(rows)]  # Example cost matrix\n",
        "\n",
        "walls = [[1, 1]]  # Example walls\n",
        "\n",
        "# Train and evaluate the first policy\n",
        "game_rl_1 = GridGameRL(rows, cols, rewards, success_probabilities, costs, walls)\n",
        "game_rl_1.train(episodes=10000)\n",
        "policy_1_score = game_rl_1.evaluate_policy(episodes=1000)\n",
        "print(\"Policy 1 Score:\", policy_1_score)\n",
        "\n",
        "# Train and evaluate the second policy\n",
        "game_rl_2 = GridGameRL(rows, cols, rewards, success_probabilities, costs, walls)\n",
        "game_rl_2.epsilon = 0.01  # Changing epsilon for the second policy as an example\n",
        "game_rl_2.train(episodes=10000)\n",
        "policy_2_score = game_rl_2.evaluate_policy(episodes=1000)\n",
        "print(\"Policy 2 Score:\", policy_2_score)\n",
        "\n",
        "# Compare policies\n",
        "if policy_1_score > policy_2_score:\n",
        "    print(\"Policy 1 is better.\")\n",
        "elif policy_1_score < policy_2_score:\n",
        "    print(\"Policy 2 is better.\")\n",
        "else:\n",
        "    print(\"Both policies are equally good.\")\n",
        "\n",
        "# Print policies for visual comparison\n",
        "print(\"\\nPolicy 1:\")\n",
        "game_rl_1.print_policy()\n",
        "print(\"\\nPolicy 2:\")\n",
        "game_rl_2.print_policy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifMXmITsJ8br",
        "outputId": "08ebc0f8-9c14-4750-c89d-3680cf5a272a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy 1 Score: -6.902\n",
            "Policy 2 Score: -7.028\n",
            "Policy 1 is better.\n",
            "\n",
            "Policy 1:\n",
            "→ → ↓\n",
            "↓ W ↓\n",
            "→ → ↓\n",
            "→ -1 +1\n",
            "\n",
            "Policy 2:\n",
            "↓ ← →\n",
            "↓ W →\n",
            "↓ ↓ ↓\n",
            "→ -1 +1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probabilities, costs, goal, avoid, walls, gamma=0.9, epsilon=0.1):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probabilities = success_probabilities  # Dictionary of move: success probability\n",
        "        self.costs = costs  # Cost matrix\n",
        "        self.goal = goal  # Goal location\n",
        "        self.avoid = avoid  # Avoid location\n",
        "        self.walls = walls  # List of wall locations\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Exploration rate\n",
        "        self.q_values = np.zeros((rows, cols, 4))  # Q-values for each state-action pair\n",
        "        self.returns = {}  # Dictionary to store returns for state-action pairs\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Possible moves: right, left, down, up\n",
        "\n",
        "        # Initialize returns dictionary\n",
        "        for row in range(rows):\n",
        "            for col in range(cols):\n",
        "                for action in range(4):\n",
        "                    self.returns[((row, col), action)] = []\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.choice(range(4))  # Explore: choose a random action\n",
        "        else:\n",
        "            return np.argmax(self.q_values[state[0], state[1]])  # Exploit: choose the best action\n",
        "\n",
        "    def take_action(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        success_prob = self.success_probabilities[move]\n",
        "        if random.uniform(0, 1) < success_prob:\n",
        "            next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "            if not self.is_valid_location(next_state):\n",
        "                next_state = state  # Invalid move: stay in the same state\n",
        "        else:\n",
        "            next_state = state  # Move failed: stay in the same state\n",
        "        return next_state\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = [0, 0]  # Start at the initial position\n",
        "        while state != self.goal and state != self.avoid:\n",
        "            action = self.choose_action(state)\n",
        "            next_state = self.take_action(state, action)\n",
        "            reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "        return episode\n",
        "\n",
        "    def update_q_values(self, episode):\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            state, action, reward = episode[t]\n",
        "            G = self.gamma * G + reward\n",
        "            if (tuple(state), action) not in visited:\n",
        "                self.returns[(tuple(state), action)].append(G)\n",
        "                self.q_values[state[0], state[1], action] = np.mean(self.returns[(tuple(state), action)])\n",
        "                visited.add((tuple(state), action))\n",
        "\n",
        "    def train(self, episodes=1000):\n",
        "        for episode_num in range(episodes):\n",
        "            episode = self.generate_episode()\n",
        "            self.update_q_values(episode)\n",
        "\n",
        "    def evaluate_policy(self, episodes=100):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while state != self.goal and state != self.avoid:\n",
        "                action = self.choose_action(state)\n",
        "                next_state = self.take_action(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] == self.goal:\n",
        "                    policy[row][col] = 'G'\n",
        "                elif [row, col] == self.avoid:\n",
        "                    policy[row][col] = 'X'\n",
        "                elif [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'\n",
        "                else:\n",
        "                    best_action = np.argmax(self.q_values[row, col])\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "# Example setup:\n",
        "rows = 5\n",
        "cols = 5\n",
        "rewards = [[0 for _ in range(cols)] for _ in range(rows)]  # Example reward matrix\n",
        "rewards[4][4] = 10  # Goal reward\n",
        "rewards[2][2] = -10  # Avoid penalty\n",
        "\n",
        "success_probabilities = {\n",
        "    (0, 1): 0.8,  # 80% chance to move right\n",
        "    (0, -1): 0.8, # 80% chance to move left\n",
        "    (1, 0): 0.8,  # 80% chance to move down\n",
        "    (-1, 0): 0.8  # 80% chance to move up\n",
        "}\n",
        "\n",
        "costs = [[-1 for _ in range(cols)] for _ in range(rows)]  # Example cost matrix\n",
        "\n",
        "goal = [4, 4]\n",
        "avoid = [2, 2]\n",
        "walls = [[1, 1], [1, 2], [2, 1]]  # Example walls\n",
        "\n",
        "# Train and evaluate the first policy\n",
        "game_rl_1 = GridGameRL(rows, cols, rewards, success_probabilities, costs, goal, avoid, walls)\n",
        "game_rl_1.train(episodes=10000)\n",
        "policy_1_score = game_rl_1.evaluate_policy(episodes=1000)\n",
        "print(\"Policy 1 Score:\", policy_1_score)\n",
        "\n",
        "# Train and evaluate the second policy\n",
        "game_rl_2 = GridGameRL(rows, cols, rewards, success_probabilities, costs, goal, avoid, walls)\n",
        "game_rl_2.epsilon = 0.2  # Changing epsilon for the second policy as an example\n",
        "game_rl_2.train(episodes=10000)\n",
        "policy_2_score = game_rl_2.evaluate_policy(episodes=1000)\n",
        "print(\"Policy 2 Score:\", policy_2_score)\n",
        "\n",
        "# Compare policies\n",
        "if policy_1_score > policy_2_score:\n",
        "    print(\"Policy 1 is better.\")\n",
        "elif policy_1_score < policy_2_score:\n",
        "    print(\"Policy 2 is better.\")\n",
        "else:\n",
        "    print(\"Both policies are equally good.\")\n",
        "\n",
        "# Print policies for visual comparison\n",
        "print(\"\\nPolicy 1:\")\n",
        "game_rl_1.print_policy()\n",
        "print(\"\\nPolicy 2:\")\n",
        "game_rl_2.print_policy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reW5MoRMPsZh",
        "outputId": "b565c448-bf52-47f2-df88-eddeb441ccde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy 1 Score: -8.585\n",
            "Policy 2 Score: -9.632\n",
            "Policy 1 is better.\n",
            "\n",
            "Policy 1:\n",
            "↓ ← ↓ ↓ ↑\n",
            "↓ W W ↓ ↓\n",
            "↓ W X ← ←\n",
            "→ → ↑ ↑ ↓\n",
            "→ → ↑ → G\n",
            "\n",
            "Policy 2:\n",
            "↓ ← → ↓ →\n",
            "↓ W W ↓ ↓\n",
            "↓ W X ← ←\n",
            "→ → ↑ → ↓\n",
            "→ → → → G\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "# first try of RL\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probabilities, costs, walls, gamma=0.9):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probabilities = success_probabilities  # Dictionary of move: success probability\n",
        "        self.costs = costs  # Cost matrix\n",
        "        self.walls = walls  # List of wall locations\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.value_table = np.zeros((rows, cols))  # Value function for each state\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)  # Policy for each state\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Possible moves: right, left, down, up\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        success_prob = self.success_probabilities[move]\n",
        "        if random.uniform(0, 1) < success_prob:\n",
        "            next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "            if not self.is_valid_location(next_state):\n",
        "                next_state = state  # Invalid move: stay in the same state\n",
        "        else:\n",
        "            next_state = state  # Move failed: stay in the same state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        next_state = self.get_next_state([row, col], action)\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "                        value = reward + self.gamma * self.value_table[next_state[0], next_state[1]]\n",
        "                        if value > new_value:\n",
        "                            new_value = value\n",
        "                            self.policy[row, col] = action\n",
        "                    self.value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=100):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while self.rewards[state[0]][state[1]] == 0:  # Continue until reaching a terminal state\n",
        "                action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "# Example setup:\n",
        "rows = 5\n",
        "cols = 5\n",
        "rewards = [[0 for _ in range(cols)] for _ in range(rows)]  # Example reward matrix\n",
        "rewards[2][3] = 1  # Positive reward\n",
        "rewards[4][4] = -1  # Negative reward\n",
        "\n",
        "success_probabilities = {\n",
        "    (0, 1): 0.8,  # 80% chance to move right\n",
        "    (0, -1): 0.8, # 80% chance to move left\n",
        "    (1, 0): 0.8,  # 80% chance to move down\n",
        "    (-1, 0): 0.8  # 80% chance to move up\n",
        "}\n",
        "\n",
        "costs = [[-1 for _ in range(cols)] for _ in range(rows)]  # Example cost matrix\n",
        "\n",
        "walls = [[1, 1], [1, 2], [2, 1]]  # Example walls\n",
        "\n",
        "# Train and evaluate the policy\n",
        "game_mbrl = GridGameMBRL(rows, cols, rewards, success_probabilities, costs, walls)\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "game_mbrl.value_iteration()\n",
        "end_time = time.time()\n",
        "\n",
        "policy_score = game_mbrl.evaluate_policy(episodes=1000)\n",
        "print(\"Policy Score:\", policy_score)\n",
        "print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Print policy for visual comparison\n",
        "print(\"\\nPolicy:\")\n",
        "game_mbrl.print_policy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "BlRmExOLOr2a",
        "outputId": "e317a6da-f08f-4c71-a874-9fb842d6079a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-61e1bda1ac6c>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mgame_mbrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-61e1bda1ac6c>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(self, theta)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probabilities, costs, walls, gamma=0.9):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probabilities = success_probabilities  # Dictionary of move: success probability\n",
        "        self.costs = costs  # Cost matrix\n",
        "        self.walls = walls  # List of wall locations\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.value_table = np.zeros((rows, cols))  # Value function for each state\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)  # Policy for each state\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Possible moves: right, left, down, up\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state  # Invalid move: stay in the same state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        next_state = self.get_next_state([row, col], action)\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "                        value = reward + self.gamma * self.value_table[next_state[0], next_state[1]]\n",
        "                        if value > new_value:\n",
        "                            new_value = value\n",
        "                            self.policy[row, col] = action\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while self.rewards[state[0]][state[1]] == 0:  # Continue until reaching a terminal state\n",
        "                action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "# Example setup:\n",
        "rows = 5\n",
        "cols = 5\n",
        "rewards = [[0 for _ in range(cols)] for _ in range(rows)]  # Example reward matrix\n",
        "rewards[2][3] = 1  # Positive reward\n",
        "rewards[4][4] = -1  # Negative reward\n",
        "\n",
        "success_probabilities = {\n",
        "    (0, 1): 0.8,  # 80% chance to move right\n",
        "    (0, -1): 0.8, # 80% chance to move left\n",
        "    (1, 0): 0.8,  # 80% chance to move down\n",
        "    (-1, 0): 0.8  # 80% chance to move up\n",
        "}\n",
        "\n",
        "costs = [[-1 for _ in range(cols)] for _ in range(rows)]  # Example cost matrix\n",
        "\n",
        "walls = [[1, 1], [1, 2], [2, 1]]  # Example walls\n",
        "\n",
        "# Train and evaluate the policy\n",
        "game_mbrl = GridGameMBRL(rows, cols, rewards, success_probabilities, costs, walls)\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "game_mbrl.value_iteration()\n",
        "end_time = time.time()\n",
        "\n",
        "policy_score = game_mbrl.evaluate_policy(episodes=1000)\n",
        "print(\"Policy Score:\", policy_score)\n",
        "print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Print policy for visual comparison\n",
        "print(\"\\nPolicy:\")\n",
        "game_mbrl.print_policy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DSbSLrJPseV",
        "outputId": "eb614e30-1f12-4bce-cb70-46a428b62b09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Score: -5.0\n",
            "Value Iteration Time: 0.0181 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → ↓ ←\n",
            "↓ W W ↓ ←\n",
            "↓ W → → ←\n",
            "→ → → ↑ ←\n",
            "→ → → ↑ ←\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probabilities, costs, walls, gamma=0.9):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probabilities = success_probabilities  # Dictionary of move: success probability\n",
        "        self.costs = costs  # Cost matrix\n",
        "        self.walls = walls  # List of wall locations\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.value_table = np.zeros((rows, cols))  # Value function for each state\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)  # Policy for each state\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]  # Possible moves: right, left, down, up\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state  # Invalid move: stay in the same state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        next_state = self.get_next_state([row, col], action)\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "                        value = reward + self.gamma * self.value_table[next_state[0], next_state[1]]\n",
        "                        if value > new_value:\n",
        "                            new_value = value\n",
        "                            self.policy[row, col] = action\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while self.rewards[state[0]][state[1]] == 0:  # Continue until reaching a terminal state\n",
        "                action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "def main():\n",
        "    # Set up the grid parameters\n",
        "    w = 12\n",
        "    h = 4\n",
        "    L = [(1,0,-100),(2,0,-100),(3,0,-100),(4,0,-100),(5,0,-100),(6,0,-100),(7,0,-100),(8,0,-100),(9,0,-100),(10,0,-100),(11,0,0)]\n",
        "    p = 1\n",
        "    r = -1\n",
        "\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([y, x])  # Note the (x, y) to (row, col) conversion\n",
        "        else:\n",
        "            rewards[y][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probabilities = {\n",
        "        (0, 1): p,  # 80% chance to move right\n",
        "        (0, -1): p, # 80% chance to move left\n",
        "        (1, 0): p,  # 80% chance to move down\n",
        "        (-1, 0): p  # 80% chance to move up\n",
        "    }\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probabilities, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000)\n",
        "    print(\"Policy Score:\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "j0W9x4imccAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "82d7204c-943f-4290-9d60-89dfe40c13cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-63755f351a4e>\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-63755f351a4e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;31m# Evaluate the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mpolicy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_mbrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Policy Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Value Iteration Time: {end_time - start_time:.4f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-63755f351a4e>\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Continue until reaching a terminal state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-63755f351a4e>\u001b[0m in \u001b[0;36mget_next_state\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 location not in self.walls)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmove\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probabilities, costs, walls, gamma=0.9):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probabilities = success_probabilities\n",
        "        self.costs = costs\n",
        "        self.walls = walls\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros((rows, cols))\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        next_state = self.get_next_state([row, col], action)\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "                        value = reward + self.gamma * self.value_table[next_state[0], next_state[1]]\n",
        "                        if value > new_value:\n",
        "                            new_value = value\n",
        "                            self.policy[row, col] = action\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000, epsilon=0.1):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while self.rewards[state[0]][state[1]] == 0:\n",
        "                if random.uniform(0, 1) < epsilon:\n",
        "                    action = random.choice(range(4))\n",
        "                else:\n",
        "                    action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = 'P'  # Positive reward\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = 'N'  # Negative reward\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "def main():\n",
        "    # Set up the grid parameters\n",
        "    w = 12\n",
        "    h = 4\n",
        "    L = [(1,0,-100),(2,0,-100),(3,0,-100),(4,0,-100),(5,0,-100),(6,0,-100),(7,0,-100),(8,0,-100),(9,0,-100),(10,0,-100),(11,0,0)]\n",
        "    p = 1\n",
        "    r = -1\n",
        "\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([y, x])  # Note the (x, y) to (row, col) conversion\n",
        "        else:\n",
        "            rewards[y][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probabilities = {\n",
        "        (0, 1): p,\n",
        "        (0, -1): p,\n",
        "        (1, 0): p,\n",
        "        (-1, 0): p\n",
        "    }\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probabilities, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000, epsilon=0.1)\n",
        "    print(\"Policy Score:\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGeLnlkVBkfi",
        "outputId": "25e9b12f-b764-47aa-dde9-9f6b4ed27103"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Score: -1726.545\n",
            "Value Iteration Time: 0.0305 seconds\n",
            "\n",
            "Policy:\n",
            "← N N N N N N N N N N W\n",
            "→ → → → → → → → → → → →\n",
            "→ → → → → → → → → → → →\n",
            "→ → → → → → → → → → → →\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TMoHC7hXZecU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}