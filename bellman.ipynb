{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMR41ladyImOILtqYVH4F2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matanaaa14/ai_task2/blob/main/bellman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5usBOhEKC_h",
        "outputId": "609f8328-856b-4e80-ce11-5acfb42c9223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Score: -0.21535999999999778\n",
            "Value Iteration Time: 0.0181 seconds\n",
            "\n",
            "Policy:\n",
            "→ → ↓ ↓\n",
            "↓ W ↓ N\n",
            "→ → → P\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probability, costs, walls, gamma=0.9):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probability = success_probability\n",
        "        self.costs = costs\n",
        "        self.walls = walls\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros((rows, cols))\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        action_value = 0\n",
        "                        main_prob = self.success_probability\n",
        "                        side_prob = (1 - main_prob) / 2\n",
        "\n",
        "                        # Define the actions for main, left, and right\n",
        "                        main_action = action\n",
        "                        left_action = (action + 1) % 4\n",
        "                        right_action = (action - 1) % 4\n",
        "\n",
        "                        # Get the next states\n",
        "                        next_state_main = self.get_next_state([row, col], main_action)\n",
        "                        next_state_left = self.get_next_state([row, col], left_action)\n",
        "                        next_state_right = self.get_next_state([row, col], right_action)\n",
        "\n",
        "                        # Calculate the action value considering the probabilities\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "                        action_value += main_prob * (reward + self.gamma * self.value_table[next_state_main[0], next_state_main[1]])\n",
        "                        action_value += side_prob * (reward + self.gamma * self.value_table[next_state_left[0], next_state_left[1]])\n",
        "                        action_value += side_prob * (reward + self.gamma * self.value_table[next_state_right[0], next_state_right[1]])\n",
        "\n",
        "                        # Update the new value and policy\n",
        "                        if action_value > new_value:\n",
        "                            new_value = action_value\n",
        "                            self.policy[row, col] = action\n",
        "\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000, epsilon=0.1):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while self.rewards[state[0]][state[1]] == 0:\n",
        "                if random.uniform(0, 1) < epsilon:\n",
        "                    action = random.choice(range(4))\n",
        "                else:\n",
        "                    action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = 'P'  # Positive reward\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = 'N'  # Negative reward\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "def main():\n",
        "    # Set up the grid parameters\n",
        "    w = 4\n",
        "    h = 3\n",
        "    L = [(1, 1, 0), (3, 2, 1), (3, 1, -1)]\n",
        "    p = 0.9\n",
        "    r = -0.04\n",
        "\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([y, x])  # Note the (x, y) to (row, col) conversion\n",
        "        else:\n",
        "            rewards[y][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probability = p\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probability, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000, epsilon=0.1)\n",
        "    print(\"Policy Score:\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probability, costs, walls, gamma=0.9):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probability = success_probability\n",
        "        self.costs = costs\n",
        "        self.walls = walls\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros((rows, cols))\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        action_value = 0\n",
        "                        main_prob = self.success_probability\n",
        "                        side_prob = (1 - main_prob) / 2\n",
        "\n",
        "                        main_action = action\n",
        "                        left_action = (action + 1) % 4\n",
        "                        right_action = (action - 1) % 4\n",
        "\n",
        "                        next_state_main = self.get_next_state([row, col], main_action)\n",
        "                        next_state_left = self.get_next_state([row, col], left_action)\n",
        "                        next_state_right = self.get_next_state([row, col], right_action)\n",
        "\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "\n",
        "                        main_value = reward + self.gamma * self.value_table[next_state_main[0], next_state_main[1]]\n",
        "                        left_value = reward + self.gamma * self.value_table[next_state_left[0], next_state_left[1]]\n",
        "                        right_value = reward + self.gamma * self.value_table[next_state_right[0], next_state_right[1]]\n",
        "\n",
        "                        if self.rewards[next_state_main[0]][next_state_main[1]] < 0:\n",
        "                            main_value += self.rewards[next_state_main[0]][next_state_main[1]]\n",
        "                        if self.rewards[next_state_left[0]][next_state_left[1]] < 0:\n",
        "                            left_value += self.rewards[next_state_left[0]][next_state_left[1]]\n",
        "                        if self.rewards[next_state_right[0]][next_state_right[1]] < 0:\n",
        "                            right_value += self.rewards[next_state_right[0]][next_state_right[1]]\n",
        "\n",
        "                        action_value += main_prob * main_value\n",
        "                        action_value += side_prob * left_value\n",
        "                        action_value += side_prob * right_value\n",
        "\n",
        "                        if action_value > new_value:\n",
        "                            new_value = action_value\n",
        "                            self.policy[row, col] = action\n",
        "\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000, epsilon=0.1):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while self.rewards[state[0]][state[1]] == 0:\n",
        "                if random.uniform(0, 1) < epsilon:\n",
        "                    action = random.choice(range(4))\n",
        "                else:\n",
        "                    action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = 'P'  # Positive reward\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = 'N'  # Negative reward\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "def main():\n",
        "    # Set up the grid parameters\n",
        "    w = 4\n",
        "    h = 3\n",
        "    L = [(1,1,0),(3,2,1),(3,1,-1)]\n",
        "    p = 0.8\n",
        "    r = -0.1\n",
        "\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([h - y - 1, x])  # Convert (x, y) to (row, col) with (0,0) at lower-left\n",
        "        else:\n",
        "            rewards[h - y - 1][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probability = p\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probability, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000, epsilon=0.1)\n",
        "    print(\"Policy Score:\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TCglL7lKDsb",
        "outputId": "03a1fbeb-8d1c-4575-9aa0-cdc5965dbd83"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Score: -0.335000000000005\n",
            "Value Iteration Time: 0.0962 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → P\n",
            "↑ W ↑ N\n",
            "↑ → ↑ ←\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lG18OuZRfhcv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}