{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwghhrvXBPEgayHsqUqKZR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matanaaa14/ai_task2/blob/main/bellman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5usBOhEKC_h",
        "outputId": "609f8328-856b-4e80-ce11-5acfb42c9223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Score: -0.21535999999999778\n",
            "Value Iteration Time: 0.0181 seconds\n",
            "\n",
            "Policy:\n",
            "→ → ↓ ↓\n",
            "↓ W ↓ N\n",
            "→ → → P\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probability, costs, walls, gamma=0.9):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probability = success_probability\n",
        "        self.costs = costs\n",
        "        self.walls = walls\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros((rows, cols))\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        action_value = 0\n",
        "                        main_prob = self.success_probability\n",
        "                        side_prob = (1 - main_prob) / 2\n",
        "\n",
        "                        # Define the actions for main, left, and right\n",
        "                        main_action = action\n",
        "                        left_action = (action + 1) % 4\n",
        "                        right_action = (action - 1) % 4\n",
        "\n",
        "                        # Get the next states\n",
        "                        next_state_main = self.get_next_state([row, col], main_action)\n",
        "                        next_state_left = self.get_next_state([row, col], left_action)\n",
        "                        next_state_right = self.get_next_state([row, col], right_action)\n",
        "\n",
        "                        # Calculate the action value considering the probabilities\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "                        action_value += main_prob * (reward + self.gamma * self.value_table[next_state_main[0], next_state_main[1]])\n",
        "                        action_value += side_prob * (reward + self.gamma * self.value_table[next_state_left[0], next_state_left[1]])\n",
        "                        action_value += side_prob * (reward + self.gamma * self.value_table[next_state_right[0], next_state_right[1]])\n",
        "\n",
        "                        # Update the new value and policy\n",
        "                        if action_value > new_value:\n",
        "                            new_value = action_value\n",
        "                            self.policy[row, col] = action\n",
        "\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000, epsilon=0.1):\n",
        "        total_return = 0\n",
        "        for _ in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            while self.rewards[state[0]][state[1]] == 0:\n",
        "                if random.uniform(0, 1) < epsilon:\n",
        "                    action = random.choice(range(4))\n",
        "                else:\n",
        "                    action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                state = next_state\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = 'P'  # Positive reward\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = 'N'  # Negative reward\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "def main():\n",
        "    # Set up the grid parameters\n",
        "    w = 4\n",
        "    h = 3\n",
        "    L = [(1, 1, 0), (3, 2, 1), (3, 1, -1)]\n",
        "    p = 0.9\n",
        "    r = -0.04\n",
        "\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([y, x])  # Note the (x, y) to (row, col) conversion\n",
        "        else:\n",
        "            rewards[y][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probability = p\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probability, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000, epsilon=0.1)\n",
        "    print(\"Policy Score:\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probability, costs, walls, gamma=0.5):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probability = success_probability\n",
        "        self.costs = costs\n",
        "        self.walls = walls\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros((rows, cols))\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        action_value = 0\n",
        "                        main_prob = self.success_probability\n",
        "                        side_prob = (1 - main_prob) / 2\n",
        "\n",
        "                        main_action = action\n",
        "                        left_action = (action + 1) % 4\n",
        "                        right_action = (action - 1) % 4\n",
        "\n",
        "                        next_state_main = self.get_next_state([row, col], main_action)\n",
        "                        next_state_left = self.get_next_state([row, col], left_action)\n",
        "                        next_state_right = self.get_next_state([row, col], right_action)\n",
        "\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "\n",
        "                        main_value = reward + self.gamma * self.value_table[next_state_main[0], next_state_main[1]]\n",
        "                        left_value = reward + self.gamma * self.value_table[next_state_left[0], next_state_left[1]]\n",
        "                        right_value = reward + self.gamma * self.value_table[next_state_right[0], next_state_right[1]]\n",
        "\n",
        "                        if self.rewards[next_state_main[0]][next_state_main[1]] < 0:\n",
        "                            main_value = max(main_value, self.rewards[next_state_main[0]][next_state_main[1]])\n",
        "                        if self.rewards[next_state_left[0]][next_state_left[1]] < 0:\n",
        "                            left_value = max(left_value, self.rewards[next_state_left[0]][next_state_left[1]])\n",
        "                        if self.rewards[next_state_right[0]][next_state_right[1]] < 0:\n",
        "                            right_value = max(right_value, self.rewards[next_state_right[0]][next_state_right[1]])\n",
        "\n",
        "                        action_value += main_prob * main_value\n",
        "                        action_value += side_prob * left_value\n",
        "                        action_value += side_prob * right_value\n",
        "\n",
        "                        if action_value > new_value:\n",
        "                            new_value = action_value\n",
        "                            self.policy[row, col] = action\n",
        "\n",
        "                    if self.costs[row][col] > 0:\n",
        "                        new_value = max(new_value, self.value_table[row][col])\n",
        "\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000, max_steps=100, epsilon=0.1):\n",
        "        total_return = 0\n",
        "        for episode in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            step = 0\n",
        "            while step < max_steps:\n",
        "                if random.uniform(0, 1) < epsilon:\n",
        "                    action = random.choice(range(4))\n",
        "                else:\n",
        "                    action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                if self.rewards[state[0]][state[1]] != 0 and self.costs[state[0]][state[1]] <= 0:\n",
        "                    break  # Terminate the episode at a terminal state with non-positive cost\n",
        "                state = next_state\n",
        "                step += 1\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = 'P'  # Positive reward\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = 'N'  # Negative reward\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "def main():\n",
        "    # Set up the grid parameters\n",
        "    w = 4\n",
        "    h = 3\n",
        "    L = [(1, 1, 0), (3, 2, 1), (3, 1, -1)]\n",
        "    p = 0.5\n",
        "    r = -0.04\n",
        "\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([h - y - 1, x])  # Convert (x, y) to (row, col) with (0,0) at lower-left\n",
        "        else:\n",
        "            rewards[h - y - 1][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probability = p\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probability, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000, max_steps=100, epsilon=0.1)\n",
        "    print(\"Policy Score:\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TCglL7lKDsb",
        "outputId": "5e750fa5-9bb7-4d5a-c58d-a95966a88150"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Score: 0.7711600000000015\n",
            "Value Iteration Time: 0.0037 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → P\n",
            "↑ W ↓ N\n",
            "↑ → ↑ ←\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probability, costs, walls, gamma=0.5):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probability = success_probability\n",
        "        self.costs = costs\n",
        "        self.walls = walls\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros((rows, cols))\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        action_value = 0\n",
        "                        main_prob = self.success_probability\n",
        "                        side_prob = (1 - main_prob) / 2\n",
        "\n",
        "                        main_action = action\n",
        "                        left_action = (action + 1) % 4\n",
        "                        right_action = (action - 1) % 4\n",
        "\n",
        "                        next_state_main = self.get_next_state([row, col], main_action)\n",
        "                        next_state_left = self.get_next_state([row, col], left_action)\n",
        "                        next_state_right = self.get_next_state([row, col], right_action)\n",
        "\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "\n",
        "                        main_value = reward + self.gamma * self.value_table[next_state_main[0], next_state_main[1]]\n",
        "                        left_value = reward + self.gamma * self.value_table[next_state_left[0], next_state_left[1]]\n",
        "                        right_value = reward + self.gamma * self.value_table[next_state_right[0], next_state_right[1]]\n",
        "\n",
        "                        if self.rewards[next_state_main[0]][next_state_main[1]] < 0:\n",
        "                            main_value = max(main_value, self.rewards[next_state_main[0]][next_state_main[1]])\n",
        "                        if self.rewards[next_state_left[0]][next_state_left[1]] < 0:\n",
        "                            left_value = max(left_value, self.rewards[next_state_left[0]][next_state_left[1]])\n",
        "                        if self.rewards[next_state_right[0]][next_state_right[1]] < 0:\n",
        "                            right_value = max(right_value, self.rewards[next_state_right[0]][next_state_right[1]])\n",
        "\n",
        "                        action_value += main_prob * main_value\n",
        "                        action_value += side_prob * left_value\n",
        "                        action_value += side_prob * right_value\n",
        "\n",
        "                        # Introduce a risk-averse factor when success probability is low\n",
        "                        risk_factor = 1 - self.success_probability\n",
        "                        if self.rewards[row][col] < 0:\n",
        "                            action_value -= risk_factor * abs(self.rewards[row][col])\n",
        "\n",
        "                        if action_value > new_value:\n",
        "                            new_value = action_value\n",
        "                            self.policy[row, col] = action\n",
        "\n",
        "                    if self.costs[row][col] > 0:\n",
        "                        new_value = max(new_value, self.value_table[row][col])\n",
        "\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000, max_steps=100, epsilon=0.1):\n",
        "        total_return = 0\n",
        "        for episode in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            step = 0\n",
        "            while step < max_steps:\n",
        "                if random.uniform(0, 1) < epsilon:\n",
        "                    action = random.choice(range(4))\n",
        "                else:\n",
        "                    action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                if self.rewards[state[0]][state[1]] != 0 and self.costs[state[0]][state[1]] <= 0:\n",
        "                    break  # Terminate the episode at a terminal state with non-positive cost\n",
        "                state = next_state\n",
        "                step += 1\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = 'P'  # Positive reward\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = 'N'  # Negative reward\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "def main():\n",
        "    # Set up the grid parameters\n",
        "    w = 12\n",
        "    h = 6\n",
        "    L = [(1, 0, -100), (2, 0, -100), (3, 0, -100), (4, 0, -100), (5, 0, -100),\n",
        "         (6, 0, -100), (7, 0, -100), (8, 0, -100), (9, 0, -100), (10, 0, -100), (11, 0, 1)]\n",
        "    p = 0.55\n",
        "    r = -1\n",
        "\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([h - y - 1, x])  # Convert (x, y) to (row, col) with (0,0) at lower-left\n",
        "        else:\n",
        "            rewards[h - y - 1][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probability = p\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probability, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000, max_steps=100, epsilon=0.1)\n",
        "    print(\"Policy Score (Value Iteration):\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "GRN91W4OPMZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530a2378-4028-431b-951d-8271387996eb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Score (Value Iteration): -17.71\n",
            "Value Iteration Time: 0.0429 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → → → → → → → → → ↓\n",
            "→ → → → → → → → → → → ↓\n",
            "→ → → → → → → → → → → ↓\n",
            "↓ → → → → → → → → → → ↓\n",
            "→ → → → → → → → → → → ↓\n",
            "↓ N N N N N N N N N N P\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tabulate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNnVMsQu0-R3",
        "outputId": "d3585210-d7b2-49c0-fa7a-b9ce745fbbeb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tabulate import tabulate\n",
        "\n",
        "class GridGameMBRL:\n",
        "    def __init__(self, rows, cols, rewards, success_probability, costs, walls, gamma=0.5):\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.rewards = rewards\n",
        "        self.success_probability = success_probability\n",
        "        self.costs = costs\n",
        "        self.walls = walls\n",
        "        self.gamma = gamma\n",
        "        self.value_table = np.zeros((rows, cols))\n",
        "        self.policy = np.zeros((rows, cols), dtype=int)\n",
        "        self.actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
        "        self.safety_distance = 2  # Safety distance to negative rewards\n",
        "\n",
        "    def is_valid_location(self, location):\n",
        "        return (0 <= location[0] < self.rows and\n",
        "                0 <= location[1] < self.cols and\n",
        "                location not in self.walls)\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        move = self.actions[action]\n",
        "        next_state = [state[0] + move[0], state[1] + move[1]]\n",
        "\n",
        "        if not self.is_valid_location(next_state):\n",
        "            next_state = state\n",
        "\n",
        "        # Check if next state is within safety distance of any negative reward\n",
        "        if self.success_probability < 0.5:\n",
        "            for dr in range(-self.safety_distance, self.safety_distance + 1):\n",
        "                for dc in range(-self.safety_distance, self.safety_distance + 1):\n",
        "                    neighbor_row = next_state[0] + dr\n",
        "                    neighbor_col = next_state[1] + dc\n",
        "                    if (0 <= neighbor_row < self.rows and\n",
        "                        0 <= neighbor_col < self.cols and\n",
        "                        self.rewards[neighbor_row][neighbor_col] < 0):\n",
        "                        # Avoid moving towards negative reward if success_probability is low\n",
        "                        if random.uniform(0, 1) > self.success_probability:\n",
        "                            return state  # Stay in the current state\n",
        "\n",
        "        return next_state\n",
        "\n",
        "    def value_iteration(self, theta=0.0001):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            new_value_table = np.copy(self.value_table)\n",
        "            for row in range(self.rows):\n",
        "                for col in range(self.cols):\n",
        "                    if [row, col] in self.walls:\n",
        "                        continue\n",
        "                    old_value = self.value_table[row, col]\n",
        "                    new_value = float('-inf')\n",
        "                    for action in range(4):\n",
        "                        action_value = 0\n",
        "                        main_prob = self.success_probability\n",
        "                        side_prob = (1 - main_prob) / 2\n",
        "\n",
        "                        main_action = action\n",
        "                        left_action = (action + 1) % 4\n",
        "                        right_action = (action - 1) % 4\n",
        "\n",
        "                        next_state_main = self.get_next_state([row, col], main_action)\n",
        "                        next_state_left = self.get_next_state([row, col], left_action)\n",
        "                        next_state_right = self.get_next_state([row, col], right_action)\n",
        "\n",
        "                        reward = self.rewards[row][col] + self.costs[row][col]\n",
        "\n",
        "                        main_value = reward + self.gamma * self.value_table[next_state_main[0], next_state_main[1]]\n",
        "                        left_value = reward + self.gamma * self.value_table[next_state_left[0], next_state_left[1]]\n",
        "                        right_value = reward + self.gamma * self.value_table[next_state_right[0], next_state_right[1]]\n",
        "\n",
        "                        if self.rewards[next_state_main[0]][next_state_main[1]] < 0:\n",
        "                            main_value = max(main_value, self.rewards[next_state_main[0]][next_state_main[1]])\n",
        "                        if self.rewards[next_state_left[0]][next_state_left[1]] < 0:\n",
        "                            left_value = max(left_value, self.rewards[next_state_left[0]][next_state_left[1]])\n",
        "                        if self.rewards[next_state_right[0]][next_state_right[1]] < 0:\n",
        "                            right_value = max(right_value, self.rewards[next_state_right[0]][next_state_right[1]])\n",
        "\n",
        "                        action_value += main_prob * main_value\n",
        "                        action_value += side_prob * left_value\n",
        "                        action_value += side_prob * right_value\n",
        "\n",
        "                        # Introduce a risk-averse factor when success probability is low\n",
        "                        risk_factor = 1 - self.success_probability\n",
        "                        if self.rewards[row][col] < 0:\n",
        "                            action_value -= risk_factor * abs(self.rewards[row][col])\n",
        "\n",
        "                        if action_value > new_value:\n",
        "                            new_value = action_value\n",
        "                            self.policy[row, col] = action\n",
        "\n",
        "                    if self.costs[row][col] > 0:\n",
        "                        new_value = max(new_value, self.value_table[row][col])\n",
        "\n",
        "                    new_value_table[row, col] = new_value\n",
        "                    delta = max(delta, abs(old_value - new_value))\n",
        "            self.value_table = new_value_table\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "    def evaluate_policy(self, episodes=10000, max_steps=100, epsilon=0.1):\n",
        "        total_return = 0\n",
        "        for episode in range(episodes):\n",
        "            state = [0, 0]\n",
        "            episode_return = 0\n",
        "            step = 0\n",
        "            while step < max_steps:\n",
        "                if random.uniform(0, 1) < epsilon:\n",
        "                    action = random.choice(range(4))\n",
        "                else:\n",
        "                    action = self.policy[state[0], state[1]]\n",
        "                next_state = self.get_next_state(state, action)\n",
        "                reward = self.rewards[state[0]][state[1]] + self.costs[state[0]][state[1]]\n",
        "                episode_return += reward\n",
        "                if self.rewards[state[0]][state[1]] != 0 and self.costs[state[0]][state[1]] <= 0:\n",
        "                    break  # Terminate the episode at a terminal state with non-positive cost\n",
        "                state = next_state\n",
        "                step += 1\n",
        "            total_return += episode_return\n",
        "        return total_return / episodes\n",
        "\n",
        "    def print_policy(self):\n",
        "        direction_mapping = ['→', '←', '↓', '↑']\n",
        "        policy = [[None for _ in range(self.cols)] for _ in range(self.rows)]\n",
        "        for row in range(self.rows):\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    policy[row][col] = 'W'  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    policy[row][col] = 'P'  # Positive reward\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    policy[row][col] = 'N'  # Negative reward\n",
        "                else:\n",
        "                    best_action = self.policy[row, col]\n",
        "                    policy[row][col] = direction_mapping[best_action]\n",
        "        for row in policy:\n",
        "            print(\" \".join(row))\n",
        "\n",
        "    def print_utilities(self):\n",
        "        utilities_table = []\n",
        "        for row in range(self.rows):\n",
        "            row_values = []\n",
        "            for col in range(self.cols):\n",
        "                if [row, col] in self.walls:\n",
        "                    row_values.append(\"W\")  # Wall\n",
        "                elif self.rewards[row][col] > 0:\n",
        "                    row_values.append(\"{:.2f}\".format(self.rewards[row][col]))  # Positive reward value\n",
        "                elif self.rewards[row][col] < 0:\n",
        "                    row_values.append(\"{:.2f}\".format(self.rewards[row][col]))  # Negative reward value\n",
        "                else:\n",
        "                    row_values.append(\"{:.2f}\".format(self.value_table[row][col]))  # Utility value\n",
        "            utilities_table.append(row_values)\n",
        "\n",
        "        print(\"\\nUtilities:\")\n",
        "        print(tabulate(utilities_table, tablefmt=\"fancy_grid\", numalign=\"center\", stralign=\"center\"))\n",
        "\n",
        "def main():\n",
        "    # Set up the grid parameters\n",
        "    w = 7\n",
        "    h = 7\n",
        "    L = [(3,1,0),(3,5,0),(1,1,-4),(1,5,-6),(5,1,1),(5,5,4)]\n",
        "    p = 0.8\n",
        "    r = -0.25\n",
        "\n",
        "    # Initialize the rewards, costs, and walls\n",
        "    rewards = [[0 for _ in range(w)] for _ in range(h)]\n",
        "    costs = [[r for _ in range(w)] for _ in range(h)]\n",
        "    walls = []\n",
        "\n",
        "    for x, y, value in L:\n",
        "        if value == 0:\n",
        "            walls.append([h - y - 1, x])  # Convert (x, y) to (row, col) with (0,0) at lower-left\n",
        "        else:\n",
        "            rewards[h - y - 1][x] = value\n",
        "\n",
        "    # Define success probabilities\n",
        "    success_probability = p\n",
        "\n",
        "    # Create the GridGameMBRL instance\n",
        "    game_mbrl = GridGameMBRL(h, w, rewards, success_probability, costs, walls)\n",
        "\n",
        "    # Perform value iteration to find the best policy\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    game_mbrl.value_iteration()\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate the policy\n",
        "    policy_score = game_mbrl.evaluate_policy(episodes=1000, max_steps=100, epsilon=0.1)\n",
        "    print(\"Policy Score (Value Iteration):\", policy_score)\n",
        "    print(f\"Value Iteration Time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "    # Print the policy and utilities for visual comparison\n",
        "    print(\"\\nPolicy:\")\n",
        "    game_mbrl.print_policy()\n",
        "\n",
        "    # Print utilities\n",
        "    game_mbrl.print_utilities()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jkEKaamUlzi",
        "outputId": "22bbb381-aecb-47e7-c845-1e97484e0720"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Score (Value Iteration): 1.79425\n",
            "Value Iteration Time: 0.0453 seconds\n",
            "\n",
            "Policy:\n",
            "→ → → → → ↓ ↓\n",
            "↓ N ↑ W → P ←\n",
            "↓ ← → → ↑ ↑ ↑\n",
            "→ → → → ↑ ↑ ↑\n",
            "→ → → → → ↓ ↓\n",
            "↓ N ↑ W → P ←\n",
            "→ ← → → ↑ ↑ ←\n",
            "\n",
            "Utilities:\n",
            "╒═══════╤═══════╤═══════╤═══════╤═══════╤══════╤═══════╕\n",
            "│ -0.47 │ -0.42 │ -0.32 │ -0.09 │ 0.45  │ 1.7  │ 0.54  │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.5  │  -6   │ -0.42 │   W   │  1.7  │  4   │  1.7  │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.49 │ -0.49 │ -0.32 │ -0.07 │ 0.51  │ 1.63 │ 0.45  │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.48 │ -0.46 │ -0.4  │ -0.29 │ -0.04 │ 0.4  │ -0.09 │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.49 │ -0.48 │ -0.45 │ -0.38 │ -0.24 │ 0.07 │ -0.23 │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.5  │  -4   │ -0.48 │   W   │ 0.05  │  1   │ 0.05  │\n",
            "├───────┼───────┼───────┼───────┼───────┼──────┼───────┤\n",
            "│ -0.5  │ -0.5  │ -0.45 │ -0.39 │ -0.24 │ 0.05 │ -0.25 │\n",
            "╘═══════╧═══════╧═══════╧═══════╧═══════╧══════╧═══════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "heTCSsKag0fN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}